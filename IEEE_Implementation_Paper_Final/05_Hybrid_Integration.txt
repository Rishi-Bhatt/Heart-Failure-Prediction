\section{Hybrid Integration and Key Features}

\subsection{Hybrid Integration Layer}

The hybrid integration layer combines predictions from the rule-based and machine learning components. We implemented a dynamic weighting approach that adjusts the contribution of each component based on confidence scores and data completeness.

\begin{algorithm}
\caption{Hybrid Integration Algorithm}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Rule-based prediction $p_r$, ML prediction $p_m$, confidence scores $c_r$ and $c_m$, patient data completeness $d$
\STATE \textbf{Output:} Final prediction $p_f$

\STATE \textbf{function} HybridIntegration($p_r$, $p_m$, $c_r$, $c_m$, $d$)
    \STATE $w_r \gets c_r \cdot (1 - \alpha \cdot d)$  \COMMENT{$\alpha$ is a tuning parameter}
    \STATE $w_m \gets c_m \cdot d$
    \STATE $w_{sum} \gets w_r + w_m$
    \STATE $p_f \gets (w_r \cdot p_r + w_m \cdot p_m) / w_{sum}$
    
    \IF{$|p_r - p_m| > \tau$}  \COMMENT{$\tau$ is a conflict threshold}
        \STATE LogConflict($p_r$, $p_m$, $c_r$, $c_m$)
        \STATE AnalyzeConflictCause($p_r$, $p_m$)
    \ENDIF
    
    \RETURN $p_f$
\end{algorithmic}
\end{algorithm}

This approach ensures that the rule-based component has more influence when data is sparse or uncertain, while the machine learning component contributes more when data is complete and reliable.

\subsection{Temporal Forecasting}

The temporal forecasting module predicts risk trajectories over time, enabling clinicians to monitor disease progression and assess intervention effects.

\subsubsection{Mathematical Formulation}

For a patient with feature vector $\mathbf{x}_t$ at time $t$, we predict the risk at future time $t+\Delta t$ as:

\begin{equation}
    r_{t+\Delta t} = \alpha \cdot f(\mathbf{x}_t) + \beta \cdot g(\mathbf{x}_{t-k:t}) + (1-\alpha-\beta) \cdot h(\mathbf{x}_t, \Delta t)
\end{equation}

where:
\begin{itemize}
    \item $f(\mathbf{x}_t)$ is the current risk prediction from the hybrid model
    \item $g(\mathbf{x}_{t-k:t})$ is a trend extrapolation based on historical data
    \item $h(\mathbf{x}_t, \Delta t)$ is a time-dependent risk adjustment
    \item $\alpha$ and $\beta$ are adaptive weights based on data quality and completeness
\end{itemize}

\begin{lstlisting}[caption=Temporal Forecasting Implementation, label=lst:temporal]
def predict_risk_trajectory(patient_id, time_points):
    # Get patient history
    history = get_patient_history(patient_id)
    
    # Calculate baseline risk
    current_risk = hybrid_model.predict(history[-1])
    
    # Extract temporal features
    risk_slope = calculate_risk_slope(history)
    risk_volatility = calculate_risk_volatility(history)
    
    # Initialize trajectory
    trajectory = []
    
    # Predict risk for each future time point
    for delta_t in time_points:
        # Base prediction from current state
        base_pred = current_risk
        
        # Trend extrapolation component
        trend_component = risk_slope * delta_t
        
        # Time-dependent adjustment
        time_adjustment = calculate_time_adjustment(
            history[-1], delta_t
        )
        
        # Adaptive weighting
        alpha = calculate_alpha(history, delta_t)
        beta = calculate_beta(history, delta_t)
        
        # Combined prediction
        predicted_risk = (
            alpha * base_pred + 
            beta * (base_pred + trend_component) + 
            (1 - alpha - beta) * time_adjustment
        )
        
        # Calculate confidence interval
        ci_width = risk_volatility * np.sqrt(delta_t)
        lower_bound = max(0, predicted_risk - ci_width)
        upper_bound = min(1, predicted_risk + ci_width)
        
        trajectory.append({
            'time': delta_t,
            'risk': predicted_risk,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound
        })
    
    return trajectory
\end{lstlisting}

\subsection{Scenario-Specific Insights}

The scenario-specific insights module generates personalized recommendations based on counterfactual analysis of different intervention scenarios.

\subsubsection{Counterfactual Generation}

For a patient with feature vector $\mathbf{x}$, we generate counterfactual scenarios $\mathbf{x}'$ by modifying modifiable risk factors:

\begin{equation}
    \mathbf{x}' = \mathbf{x} + \Delta \mathbf{x}
\end{equation}

where $\Delta \mathbf{x}$ represents clinically feasible interventions (e.g., blood pressure reduction, medication changes).

\subsubsection{Dynamic Feature Importance}

For each scenario, we calculate scenario-specific feature importance:

\begin{equation}
    I_j(\mathbf{x}') = \frac{\partial f(\mathbf{x}')}{\partial x'_j} \cdot x'_j
\end{equation}

This approach provides context-aware explanations that adapt to different intervention scenarios.

\begin{lstlisting}[caption=Scenario Modeling Implementation, label=lst:scenario]
def generate_scenario_insights(patient_data, scenario_type):
    # Generate counterfactual scenario
    if scenario_type == 'bp_control':
        counterfactual = modify_blood_pressure(
            patient_data, target_systolic=130
        )
    elif scenario_type == 'medication':
        counterfactual = add_medication(
            patient_data, medication='ace_inhibitor'
        )
    elif scenario_type == 'lifestyle':
        counterfactual = modify_lifestyle_factors(
            patient_data, weight_reduction=5
        )
    
    # Calculate risk for original and counterfactual
    original_risk = hybrid_model.predict(patient_data)
    counterfactual_risk = hybrid_model.predict(counterfactual)
    
    # Calculate risk reduction
    risk_reduction = original_risk - counterfactual_risk
    
    # Calculate scenario-specific feature importance
    importance = calculate_feature_importance(
        hybrid_model, counterfactual
    )
    
    # Generate natural language insights
    insights = generate_nl_explanation(
        original_risk, counterfactual_risk, 
        importance, scenario_type
    )
    
    return {
        'original_risk': original_risk,
        'counterfactual_risk': counterfactual_risk,
        'risk_reduction': risk_reduction,
        'feature_importance': importance,
        'insights': insights
    }
\end{lstlisting}

\subsection{Explainability Framework}

We implemented a multi-level explainability framework to provide interpretable predictions for different user needs:

\begin{itemize}
    \item \textbf{Global explanations}: Overall feature importance and model behavior
    \item \textbf{Local explanations}: SHAP values for individual predictions
    \item \textbf{Counterfactual explanations}: "What-if" scenarios showing how changes affect risk
    \item \textbf{Natural language explanations}: Converting technical explanations to clinical language
    \item \textbf{Visual explanations}: Interactive visualizations of risk factors and predictions
\end{itemize}

This approach addresses the "black box" problem of machine learning models, making predictions more transparent and actionable for clinical users.
