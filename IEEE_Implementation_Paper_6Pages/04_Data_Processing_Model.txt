\section{Data Processing and Model Implementation}

\subsection{Data Sources and Preprocessing}

For development and validation, we used data derived from the MIMIC-III clinical database (n=5,432), supplemented with a smaller validation cohort (n=1,245) from a local hospital registry. The data includes demographic information, clinical parameters, laboratory values, and cardiac history.

We implemented a comprehensive data preprocessing pipeline to handle the challenges of clinical data:

\begin{algorithm}
\caption{Data Preprocessing Pipeline}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Raw patient data
\STATE \textbf{Output:} Processed feature vectors

\STATE \textbf{function} PreprocessData(patientData)
    \STATE data $\gets$ ValidateData(patientData)
    \STATE data $\gets$ HandleMissingValues(data)
    \STATE data $\gets$ NormalizeValues(data)
    \STATE data $\gets$ EncodeCategorialVariables(data)
    \STATE data $\gets$ ExtractTemporalFeatures(data)
    \STATE data $\gets$ ApplyFeatureTransformations(data)
    \STATE data $\gets$ FeatureSelection(data)
    \RETURN data
\end{algorithmic}
\end{algorithm}

For missing value handling, we implemented a hierarchical approach:
\begin{enumerate}
    \item Domain-specific imputation for clinically interpretable values (e.g., normal ranges)
    \item Multiple imputation for variables with moderate missingness (10-30\%)
    \item Feature exclusion for variables with excessive missingness (>30\%)
\end{enumerate}

\subsection{NT-proBNP Processing}

Special attention was given to NT-proBNP processing due to its importance in heart failure prediction:

\begin{lstlisting}[caption=NT-proBNP Processing Implementation, label=lst:ntprobnp]
def process_nt_probnp(value, age, egfr=None):
    # Handle missing values
    if value is None or np.isnan(value):
        return None
        
    # Apply age-adjusted thresholds
    if age < 50:
        threshold = 450
    elif age <= 75:
        threshold = 900
    else:
        threshold = 1800
        
    # Log transformation for machine learning model
    log_value = np.log1p(value)
    
    # Sigmoid normalization for rule-based model
    normalized_value = 1.0 / (1.0 + np.exp(-0.003 * (value - threshold)))
    
    # Create interaction terms if eGFR is available
    if egfr is not None:
        # Adjust for kidney function
        if egfr < 60:  # Reduced kidney function
            kidney_factor = 1.0 + (60 - egfr) / 60
            normalized_value = normalized_value / kidney_factor
            
    return {
        'log_value': log_value,
        'normalized_value': normalized_value,
        'threshold': threshold
    }
\end{lstlisting}

\subsection{Hybrid Model Implementation}

We implemented a hybrid prediction model that combines three components:

\begin{enumerate}
    \item \textbf{Rule-Based Component}: Encodes established clinical guidelines and expert knowledge
    \item \textbf{Logistic Regression Component}: A clinically informed logistic regression model
    \item \textbf{Random Forest Component}: A more complex machine learning model
\end{enumerate}

The hybrid model dynamically combines predictions from these components using a weighted ensemble approach:

\begin{lstlisting}[caption=Hybrid Model Implementation, label=lst:hybrid]
class HybridHeartFailurePredictor:
    def __init__(self):
        """Initialize the hybrid prediction model"""
        self.ensemble_weights = self._load_weights()
        self.ml_model_available = self._check_ml_model()
        self.rf_model_available = self._check_rf_model()
        
    def predict(self, patient_data):
        """Make a prediction using the hybrid model"""
        # Get rule-based prediction
        rule_prediction, rule_confidence, rule_shap = model_enhancer.predict_heart_failure(patient_data)
        
        # Initialize with rule-based prediction
        final_prediction = rule_prediction
        final_confidence = rule_confidence
        explanations = {
            'rule_based': {
                'prediction': rule_prediction,
                'confidence': rule_confidence,
                'shap_values': rule_shap
            }
        }
        
        # Initialize model predictions and confidences
        ml_prediction = None
        ml_confidence = None
        rf_prediction = None
        rf_confidence = None
        
        # If ML model is available, incorporate its prediction
        if self.ml_model_available:
            ml_prediction, ml_confidence, ml_explanation = clinical_ml_model.predict_heart_failure(patient_data)
            explanations['ml_model'] = {
                'prediction': ml_prediction,
                'confidence': ml_confidence,
                'explanation': ml_explanation
            }
            
        # If Random Forest model is available, incorporate its prediction
        if self.rf_model_available:
            rf_prediction, rf_confidence, rf_explanation = ml_model_extensions.predict_heart_failure(patient_data)
            explanations['random_forest'] = {
                'prediction': rf_prediction,
                'confidence': rf_confidence,
                'explanation': rf_explanation
            }
            
        # Combine predictions using ensemble weights
        weighted_sum = self.ensemble_weights['rule_based'] * rule_prediction
        weight_total = self.ensemble_weights['rule_based']
        
        # Add ML prediction if available
        if ml_prediction is not None:
            weighted_sum += self.ensemble_weights['ml_model'] * ml_prediction
            weight_total += self.ensemble_weights['ml_model']
            
        # Add Random Forest prediction if available
        if rf_prediction is not None and 'random_forest' in self.ensemble_weights:
            weighted_sum += self.ensemble_weights['random_forest'] * rf_prediction
            weight_total += self.ensemble_weights['random_forest']
            
        # Normalize the weighted sum
        if weight_total > 0:
            final_prediction = weighted_sum / weight_total
            
        # Calculate model agreement
        agreements = []
        confidences = [rule_confidence]
        
        # Add ML agreement if available
        if ml_prediction is not None:
            agreements.append(1.0 - abs(rule_prediction - ml_prediction))
            confidences.append(ml_confidence)
            
        # Add Random Forest agreement if available
        if rf_prediction is not None:
            agreements.append(1.0 - abs(rule_prediction - rf_prediction))
            confidences.append(rf_confidence)
            
            # Also add agreement between ML and RF if both available
            if ml_prediction is not None:
                agreements.append(1.0 - abs(ml_prediction - rf_prediction))
                
        # Calculate average agreement and confidence
        avg_agreement = sum(agreements) / len(agreements) if agreements else 1.0
        avg_confidence = sum(confidences) / len(confidences)
        
        # Final confidence is a combination of average confidence and agreement
        final_confidence = (avg_confidence + avg_agreement) / 2
        
        return final_prediction, final_confidence, explanations
\end{lstlisting}

This hybrid approach ensures that the system leverages the strengths of each model type while mitigating their individual weaknesses. The rule-based component provides clinical interpretability, the logistic regression model offers a good balance of simplicity and performance, and the Random Forest model captures complex non-linear relationships in the data.
