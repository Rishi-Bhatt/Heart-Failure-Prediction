\section{Model Retraining Module}

A key feature of our system is the automated model retraining capability that ensures the models remain accurate as new patient data becomes available. This section details the implementation of the model retraining module.

\subsection{Retraining Architecture}

The model retraining module consists of the following components:

\begin{itemize}
    \item \textbf{Drift Detection}: Monitors model performance on new data to detect when performance degrades
    \item \textbf{Data Collection}: Gathers new patient data for retraining
    \item \textbf{Retraining Trigger}: Initiates retraining based on data volume or detected drift
    \item \textbf{Model Updater}: Retrains models and updates model parameters
    \item \textbf{Weight Adjuster}: Updates ensemble weights based on model performance
    \item \textbf{History Tracker}: Maintains a record of training events and performance metrics
\end{itemize}

\subsection{ModelRetrainer Class Implementation}

The core of the retraining functionality is implemented in the ModelRetrainer class:

\begin{lstlisting}[caption=ModelRetrainer Class Implementation, label=lst:retrainer]
class ModelRetrainer:
    """Class for handling model retraining"""
    
    def __init__(self, model, retraining_threshold=20):
        """
        Initialize the model retrainer
        
        Parameters:
        -----------
        model : HeartFailureModel
            The heart failure prediction model
        retraining_threshold : int
            Number of new records before retraining
        """
        self.model = model
        self.retraining_threshold = retraining_threshold
        self.retraining_history_path = 'data/retraining_history.json'
        self.drift_detection_threshold = 0.1  # Performance drop threshold
        
        # Initialize or load retraining history
        self.retraining_history = self._initialize_history()
        
    def check_retraining(self):
        """
        Check if model retraining is needed
        
        Returns:
        --------
        retraining_info : dict
            Information about retraining status
        """
        # Increment records count
        self.retraining_history['records_since_last_retraining'] += 1
        self._save_retraining_history()
        
        # Check for drift
        self._detect_drift()
        
        # Determine if retraining is needed
        retraining_needed = (
            self.retraining_history['records_since_last_retraining'] >= 
            self.retraining_threshold or
            self.retraining_history['drift_detected']
        )
        
        retraining_info = {
            'retraining_needed': retraining_needed,
            'records_since_last_retraining': 
                self.retraining_history['records_since_last_retraining'],
            'retraining_threshold': self.retraining_threshold,
            'drift_detected': self.retraining_history['drift_detected']
        }
        
        # If retraining is needed, retrain the model
        if retraining_needed:
            self.retrain()
            
        return retraining_info
\end{lstlisting}

\subsection{Drift Detection Algorithm}

The drift detection algorithm monitors model performance over time and triggers retraining when performance degrades:

\begin{lstlisting}[caption=Drift Detection Implementation, label=lst:drift]
def _detect_drift(self):
    """Detect model drift by evaluating performance on recent data"""
    # Need at least two performance records to detect drift
    if len(self.retraining_history['performance_history']) < 2:
        return
        
    # Get most recent performance metrics
    latest_metrics = self.retraining_history['performance_history'][-1]['metrics']
    
    # Calculate average performance from previous records
    previous_metrics = [record['metrics'] for record in 
                        self.retraining_history['performance_history'][:-1]]
    avg_previous_accuracy = np.mean([m['accuracy'] for m in previous_metrics])
    
    # Check if current performance is significantly worse
    if avg_previous_accuracy - latest_metrics['accuracy'] > self.drift_detection_threshold:
        self.retraining_history['drift_detected'] = True
        print(f"Model drift detected! Previous accuracy: {avg_previous_accuracy:.4f}, Current: {latest_metrics['accuracy']:.4f}")
        self._save_retraining_history()
\end{lstlisting}

\subsection{Retraining Process}

The retraining process updates all model components and adjusts ensemble weights based on performance:

\begin{lstlisting}[caption=Retraining Implementation, label=lst:retrain]
def retrain(self):
    """Retrain the model using all available data"""
    print("Retraining model...")
    
    # Collect all patient data
    patient_data = self._collect_patient_data()
    
    if not patient_data or len(patient_data) < 5:
        return {
            'success': False,
            'message': 'Not enough data for retraining',
            'date': datetime.now().isoformat()
        }
        
    # Prepare data for training
    X, y = self._prepare_training_data(patient_data)
    
    if len(X) == 0 or len(y) == 0:
        return {
            'success': False,
            'message': 'Failed to prepare training data',
            'date': datetime.now().isoformat()
        }
        
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)
        
    # Scale features
    X_train_scaled = self.model.scaler.fit_transform(X_train)
    X_test_scaled = self.model.scaler.transform(X_test)
    
    # Train model
    self.model.model = RandomForestClassifier(n_estimators=100, random_state=42)
    self.model.model.fit(X_train_scaled, y_train)
    
    # Evaluate model
    y_pred = self.model.model.predict(X_test_scaled)
    metrics = {
        'accuracy': float(accuracy_score(y_test, y_pred)),
        'precision': float(precision_score(y_test, y_pred, zero_division=0)),
        'recall': float(recall_score(y_test, y_pred, zero_division=0)),
        'f1': float(f1_score(y_test, y_pred, zero_division=0))
    }
    
    # Save model
    joblib.dump(self.model.model, self.model.model_path)
    joblib.dump(self.model.scaler, self.model.scaler_path)
    
    # Update retraining history
    self.retraining_history['last_retraining_date'] = datetime.now().isoformat()
    self.retraining_history['retraining_count'] += 1
    self.retraining_history['records_since_last_retraining'] = 0
    self.retraining_history['drift_detected'] = False
    self.retraining_history['performance_history'].append({
        'date': datetime.now().isoformat(),
        'metrics': metrics
    })
    self._save_retraining_history()
    
    return {
        'success': True,
        'message': 'Model retrained successfully',
        'date': datetime.now().isoformat(),
        'metrics': metrics
    }
\end{lstlisting}

\subsection{Ensemble Weight Adjustment}

After retraining, the system adjusts ensemble weights based on model performance:

\begin{lstlisting}[caption=Ensemble Weight Adjustment, label=lst:weights]
def _update_ensemble_weights(self, rule_result, ml_result, rf_result, num_records):
    """Update ensemble weights based on training results"""
    # Calculate performance metrics for each model
    model_metrics = {}
    
    # ML model metrics
    if ml_result.get('success', False):
        model_metrics['ml_model'] = ml_result.get('metrics', {}).get('roc_auc', 0.5)
    else:
        model_metrics['ml_model'] = 0
    
    # Random Forest metrics
    if rf_result.get('success', False):
        model_metrics['random_forest'] = rf_result.get('metrics', {}).get('roc_auc', 0.5)
    else:
        model_metrics['random_forest'] = 0
    
    # Rule-based model gets a baseline score that decreases as we get more data
    data_factor = min(0.6, num_records / 100)  # Cap at 0.6
    model_metrics['rule_based'] = max(0.4, 0.7 - data_factor)
    
    # Calculate total metric score
    total_metric_score = sum(model_metrics.values())
    
    # Distribute weights based on metrics
    if total_metric_score > 0:
        for model in model_metrics:
            self.ensemble_weights[model] = model_metrics[model] / total_metric_score
    
    # Ensure weights sum to 1.0
    weight_sum = sum(self.ensemble_weights.values())
    if weight_sum > 0:
        for model in self.ensemble_weights:
            self.ensemble_weights[model] /= weight_sum
\end{lstlisting}

This retraining approach ensures that the system continuously improves as more patient data becomes available, adapting to changes in the patient population and clinical practices.
